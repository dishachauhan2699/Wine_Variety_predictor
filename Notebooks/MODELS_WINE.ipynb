{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":["#import required packages\n","#basics\n","import pandas as pd \n","import numpy as np\n","\n","import warnings\n","\n","\n","#viz\n","import matplotlib.pyplot as plt\n","import matplotlib.gridspec as gridspec \n","import seaborn as sns\n","from wordcloud import WordCloud ,STOPWORDS\n","from PIL import Image\n","\n","import string\n","import re    #for regex\n","import nltk\n","from nltk.corpus import stopwords\n","\n","#import spacy\n","from nltk import pos_tag\n","from nltk.stem.wordnet import WordNetLemmatizer \n","from nltk.tokenize import word_tokenize\n","\n","\n","#FeatureEngineering\n","#!pip install lightgbm\n","from lightgbm import LGBMClassifier\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer, TfidfTransformer\n","from sklearn.base import BaseEstimator, ClassifierMixin\n","from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm, decomposition, ensemble\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.metrics import log_loss\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.preprocessing import OneHotEncoder,LabelBinarizer,LabelEncoder\n","from sklearn.model_selection import train_test_split\n","\n","import textblob\n","import xgboost\n","from keras.preprocessing import text, sequence\n","from keras import layers, models, optimizers\n","\n","from textblob import TextBlob\n","from nltk.stem import PorterStemmer\n","import nltk\n","nltk.download('wordnet')\n","from textblob import Word\n","\n","\n","color = sns.color_palette()\n","sns.set_style(\"dark\")\n","eng_stopwords = set(stopwords.words(\"english\"))\n","warnings.filterwarnings(\"ignore\")\n","\n","lem = WordNetLemmatizer()\n","\n","%matplotlib inline"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":"Using TensorFlow backend.\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\kesha\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"}]},{"metadata":{"trusted":true},"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","import statsmodels.api as sm\n","from sklearn.metrics import  roc_auc_score\n","from sklearn.metrics import classification_report"],"execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["import category_encoders as ce"],"execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Create Data audit Report for categorical variables\n","def categorical_var_summary(x):\n","    Mode = x.value_counts().sort_values(ascending = False)[0:1].reset_index()\n","    return pd.Series([x.count(), (x.isnull().sum()/x.size)*100, Mode.iloc[0, 0], Mode.iloc[0, 1], \n","                          round(Mode.iloc[0, 1] * 100/x.count(), 2 ), x.unique().size], \n","                  index = ['N', 'NMISS', 'MODE', 'FREQ', 'PERCENT', 'UNIQUE'])"],"execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Create Data audit Report for continuous variables\n","def continuous_var_summary(x):\n","    return pd.Series([x.count(),  (x.isnull().sum()/x.size)*100, x.sum(), x.mean(), x.median(),  \n","                      x.std(), x.var(), x.min(), x.quantile(0.01), x.quantile(0.05),\n","                          x.quantile(0.10),x.quantile(0.25),x.quantile(0.50),x.quantile(0.75), \n","                              x.quantile(0.90),x.quantile(0.95), x.quantile(0.99),x.max()], \n","                  index = ['N', 'NMISS', 'SUM', 'MEAN','MEDIAN', 'STD', 'VAR', 'MIN', 'P1', \n","                               'P5' ,'P10' ,'P25' ,'P50' ,'P75' ,'P90' ,'P95' ,'P99' ,'MAX'])"],"execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def onehot_features(train, test, features, full=False, sparse=False, dummy_na=True):\n","\n","    features = [f for f in features if f in train.columns]\n","    for column in features:\n","        if full:\n","            categories = pd.concat([train[column], test[column]]).dropna().unique()\n","        else:\n","            categories = train[column].dropna().unique()\n","\n","        train[column] = train[column].astype(\"category\", categories=categories)\n","        test[column] = test[column].astype(\"category\", categories=categories)\n","\n","    train = pd.get_dummies(train, columns=features, dummy_na=dummy_na, sparse=sparse)\n","    test = pd.get_dummies(test, columns=features, dummy_na=dummy_na, sparse=sparse)\n","    return train, test"],"execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["train = pd.read_csv('data/train.csv')\n","test =  pd.read_csv('data/test.csv')"],"execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["train = train.drop_duplicates(['review_description','review_title']).reset_index(drop=True)"],"execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["data_cat_vars = train.loc[:, (train.dtypes == 'object')]"],"execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["cat_info = data_cat_vars.apply(categorical_var_summary).T\n","cat_info"],"execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"                        N      NMISS  \\\nuser_name           59105     23.874   \ncountry             77608  0.0425033   \nreview_title        77641          0   \nreview_description  77641          0   \ndesignation         55458    28.5712   \nprovince            77608  0.0425033   \nregion_1            65680    15.4055   \nregion_2            33814    56.4483   \nwinery              77641          0   \nvariety             77641          0   \n\n                                                                 MODE   FREQ  \\\nuser_name                                                  @vossroger  16482   \ncountry                                                            US  36269   \nreview_title              Segura Viudas NV Extra Dry Sparkling (Cava)      7   \nreview_description  Stalky aromas suggest hay and green herbs, wit...      2   \ndesignation                                                   Reserve   1301   \nprovince                                                   California  24275   \nregion_1                                                  Napa Valley   3150   \nregion_2                                                Central Coast   7053   \nwinery                                                     Testarossa    165   \nvariety                                                    Pinot Noir   9966   \n\n                   PERCENT UNIQUE  \nuser_name            27.89     16  \ncountry              46.73     39  \nreview_title          0.01  76983  \nreview_description       0  77628  \ndesignation           2.35  26425  \nprovince             31.28    359  \nregion_1               4.8   1020  \nregion_2             20.86     18  \nwinery                0.21  13786  \nvariety              12.84     28  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>N</th>\n      <th>NMISS</th>\n      <th>MODE</th>\n      <th>FREQ</th>\n      <th>PERCENT</th>\n      <th>UNIQUE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>user_name</th>\n      <td>59105</td>\n      <td>23.874</td>\n      <td>@vossroger</td>\n      <td>16482</td>\n      <td>27.89</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>country</th>\n      <td>77608</td>\n      <td>0.0425033</td>\n      <td>US</td>\n      <td>36269</td>\n      <td>46.73</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>review_title</th>\n      <td>77641</td>\n      <td>0</td>\n      <td>Segura Viudas NV Extra Dry Sparkling (Cava)</td>\n      <td>7</td>\n      <td>0.01</td>\n      <td>76983</td>\n    </tr>\n    <tr>\n      <th>review_description</th>\n      <td>77641</td>\n      <td>0</td>\n      <td>Stalky aromas suggest hay and green herbs, wit...</td>\n      <td>2</td>\n      <td>0</td>\n      <td>77628</td>\n    </tr>\n    <tr>\n      <th>designation</th>\n      <td>55458</td>\n      <td>28.5712</td>\n      <td>Reserve</td>\n      <td>1301</td>\n      <td>2.35</td>\n      <td>26425</td>\n    </tr>\n    <tr>\n      <th>province</th>\n      <td>77608</td>\n      <td>0.0425033</td>\n      <td>California</td>\n      <td>24275</td>\n      <td>31.28</td>\n      <td>359</td>\n    </tr>\n    <tr>\n      <th>region_1</th>\n      <td>65680</td>\n      <td>15.4055</td>\n      <td>Napa Valley</td>\n      <td>3150</td>\n      <td>4.8</td>\n      <td>1020</td>\n    </tr>\n    <tr>\n      <th>region_2</th>\n      <td>33814</td>\n      <td>56.4483</td>\n      <td>Central Coast</td>\n      <td>7053</td>\n      <td>20.86</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>winery</th>\n      <td>77641</td>\n      <td>0</td>\n      <td>Testarossa</td>\n      <td>165</td>\n      <td>0.21</td>\n      <td>13786</td>\n    </tr>\n    <tr>\n      <th>variety</th>\n      <td>77641</td>\n      <td>0</td>\n      <td>Pinot Noir</td>\n      <td>9966</td>\n      <td>12.84</td>\n      <td>28</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":["## REMOVING ALL THE CATEGORICAL COLUMNS WITH MORE THAN 15% MISSING VALUES"]},{"metadata":{"trusted":true},"cell_type":"code","source":["# encoding featurs with more than 50 categories is usesless\n","train.drop(columns = cat_info[((cat_info.NMISS>=15) | (cat_info.UNIQUE>=100)) & (cat_info.index!='review_description')].index.values, inplace=True) \n","data_cat_vars = train.loc[:, (train.dtypes == 'object')]"],"execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["data_cat_vars.apply(categorical_var_summary).T"],"execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"                        N      NMISS  \\\ncountry             77608  0.0425033   \nreview_description  77641          0   \nvariety             77641          0   \n\n                                                                 MODE   FREQ  \\\ncountry                                                            US  36269   \nreview_description  Stalky aromas suggest hay and green herbs, wit...      2   \nvariety                                                    Pinot Noir   9966   \n\n                   PERCENT UNIQUE  \ncountry              46.73     39  \nreview_description       0  77628  \nvariety              12.84     28  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>N</th>\n      <th>NMISS</th>\n      <th>MODE</th>\n      <th>FREQ</th>\n      <th>PERCENT</th>\n      <th>UNIQUE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>country</th>\n      <td>77608</td>\n      <td>0.0425033</td>\n      <td>US</td>\n      <td>36269</td>\n      <td>46.73</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>review_description</th>\n      <td>77641</td>\n      <td>0</td>\n      <td>Stalky aromas suggest hay and green herbs, wit...</td>\n      <td>2</td>\n      <td>0</td>\n      <td>77628</td>\n    </tr>\n    <tr>\n      <th>variety</th>\n      <td>77641</td>\n      <td>0</td>\n      <td>Pinot Noir</td>\n      <td>9966</td>\n      <td>12.84</td>\n      <td>28</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":["wine_conti_vars = train.loc[:, (train.dtypes == 'float64') | (train.dtypes == 'int64')]"],"execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["wine_conti_vars.apply(continuous_var_summary).T.round(2)"],"execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"              N  NMISS        SUM   MEAN  MEDIAN    STD      VAR   MIN    P1  \\\npoints  77641.0   0.00  6874429.0  88.54    88.0   3.14     9.87  80.0  82.0   \nprice   72356.0   6.81  2687315.0  37.14    27.0  44.63  1991.41   4.0   8.0   \n\n          P5   P10   P25   P50   P75   P90   P95    P99     MAX  \npoints  84.0  84.0  86.0  88.0  91.0  93.0  94.0   95.0   100.0  \nprice   10.0  13.0  17.0  27.0  45.0  67.0  89.0  169.0  3300.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>N</th>\n      <th>NMISS</th>\n      <th>SUM</th>\n      <th>MEAN</th>\n      <th>MEDIAN</th>\n      <th>STD</th>\n      <th>VAR</th>\n      <th>MIN</th>\n      <th>P1</th>\n      <th>P5</th>\n      <th>P10</th>\n      <th>P25</th>\n      <th>P50</th>\n      <th>P75</th>\n      <th>P90</th>\n      <th>P95</th>\n      <th>P99</th>\n      <th>MAX</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>points</th>\n      <td>77641.0</td>\n      <td>0.00</td>\n      <td>6874429.0</td>\n      <td>88.54</td>\n      <td>88.0</td>\n      <td>3.14</td>\n      <td>9.87</td>\n      <td>80.0</td>\n      <td>82.0</td>\n      <td>84.0</td>\n      <td>84.0</td>\n      <td>86.0</td>\n      <td>88.0</td>\n      <td>91.0</td>\n      <td>93.0</td>\n      <td>94.0</td>\n      <td>95.0</td>\n      <td>100.0</td>\n    </tr>\n    <tr>\n      <th>price</th>\n      <td>72356.0</td>\n      <td>6.81</td>\n      <td>2687315.0</td>\n      <td>37.14</td>\n      <td>27.0</td>\n      <td>44.63</td>\n      <td>1991.41</td>\n      <td>4.0</td>\n      <td>8.0</td>\n      <td>10.0</td>\n      <td>13.0</td>\n      <td>17.0</td>\n      <td>27.0</td>\n      <td>45.0</td>\n      <td>67.0</td>\n      <td>89.0</td>\n      <td>169.0</td>\n      <td>3300.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":["## Dropping all the rows where price was NULL from both test and train"]},{"metadata":{"trusted":true},"cell_type":"code","source":["train.drop(index = train[train.price.isna()].index, inplace=True)"],"execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["test.price.isna().sum()"],"execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"1394"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":["wine_conti_vars = train.loc[:, (train.dtypes == 'float64') | (train.dtypes == 'int64')]"],"execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["wine_conti_vars.apply(continuous_var_summary).T.round(2)"],"execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"              N  NMISS        SUM   MEAN  MEDIAN    STD      VAR   MIN    P1  \\\npoints  72356.0    0.0  6403941.0  88.51    88.0   3.15     9.90  80.0  82.0   \nprice   72356.0    0.0  2687315.0  37.14    27.0  44.63  1991.41   4.0   8.0   \n\n          P5   P10   P25   P50   P75   P90   P95    P99     MAX  \npoints  83.0  84.0  86.0  88.0  91.0  93.0  94.0   95.0   100.0  \nprice   10.0  13.0  17.0  27.0  45.0  67.0  89.0  169.0  3300.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>N</th>\n      <th>NMISS</th>\n      <th>SUM</th>\n      <th>MEAN</th>\n      <th>MEDIAN</th>\n      <th>STD</th>\n      <th>VAR</th>\n      <th>MIN</th>\n      <th>P1</th>\n      <th>P5</th>\n      <th>P10</th>\n      <th>P25</th>\n      <th>P50</th>\n      <th>P75</th>\n      <th>P90</th>\n      <th>P95</th>\n      <th>P99</th>\n      <th>MAX</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>points</th>\n      <td>72356.0</td>\n      <td>0.0</td>\n      <td>6403941.0</td>\n      <td>88.51</td>\n      <td>88.0</td>\n      <td>3.15</td>\n      <td>9.90</td>\n      <td>80.0</td>\n      <td>82.0</td>\n      <td>83.0</td>\n      <td>84.0</td>\n      <td>86.0</td>\n      <td>88.0</td>\n      <td>91.0</td>\n      <td>93.0</td>\n      <td>94.0</td>\n      <td>95.0</td>\n      <td>100.0</td>\n    </tr>\n    <tr>\n      <th>price</th>\n      <td>72356.0</td>\n      <td>0.0</td>\n      <td>2687315.0</td>\n      <td>37.14</td>\n      <td>27.0</td>\n      <td>44.63</td>\n      <td>1991.41</td>\n      <td>4.0</td>\n      <td>8.0</td>\n      <td>10.0</td>\n      <td>13.0</td>\n      <td>17.0</td>\n      <td>27.0</td>\n      <td>45.0</td>\n      <td>67.0</td>\n      <td>89.0</td>\n      <td>169.0</td>\n      <td>3300.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":["z=train.groupby(['country'])['price','points'].mean().reset_index().sort_values('price',ascending=False)\n","z[['country','price']].head(n=10)"],"execution_count":20,"outputs":[{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"        country      price\n33  Switzerland  94.750000\n10      England  51.538462\n18        Italy  46.738333\n13      Germany  44.724688\n11       France  44.122072\n35           US  37.568786\n29       Serbia  34.666667\n5        Canada  33.470199\n17       Israel  32.914676\n2       Austria  31.970490","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>country</th>\n      <th>price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>33</th>\n      <td>Switzerland</td>\n      <td>94.750000</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>England</td>\n      <td>51.538462</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>Italy</td>\n      <td>46.738333</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Germany</td>\n      <td>44.724688</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>France</td>\n      <td>44.122072</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>US</td>\n      <td>37.568786</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>Serbia</td>\n      <td>34.666667</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Canada</td>\n      <td>33.470199</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>Israel</td>\n      <td>32.914676</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Austria</td>\n      <td>31.970490</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":["train.columns"],"execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"Index(['country', 'review_description', 'points', 'price', 'variety'], dtype='object')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":["train.dropna(subset=['country'], inplace=True)"],"execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["\n","OHE = ce.OneHotEncoder(cols=['country'],use_cat_names=True)\n","# encode the categorical variables\n","train = OHE.fit_transform(train)\n"],"execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["train.columns"],"execution_count":26,"outputs":[{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"Index(['country_Australia', 'country_US', 'country_Italy', 'country_France',\n       'country_Argentina', 'country_New Zealand', 'country_Austria',\n       'country_Chile', 'country_Portugal', 'country_Germany', 'country_Spain',\n       'country_England', 'country_Brazil', 'country_South Africa',\n       'country_Romania', 'country_Slovenia', 'country_Greece',\n       'country_Canada', 'country_Israel', 'country_Turkey', 'country_Lebanon',\n       'country_Mexico', 'country_Uruguay', 'country_Bulgaria',\n       'country_Hungary', 'country_Moldova', 'country_Morocco',\n       'country_Georgia', 'country_Ukraine', 'country_Croatia', 'country_Peru',\n       'country_Switzerland', 'country_Luxembourg', 'country_Cyprus',\n       'country_Czech Republic', 'country_Macedonia', 'country_Serbia',\n       'country_India', 'review_description', 'points', 'price', 'variety'],\n      dtype='object')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":["encode_cols = []\n","for col in train.columns:\n","    if 'country' in col:\n","        encode_cols.append(col)"],"execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["features = train.columns.difference(['variety'])\n","X = train[features]\n","y = train['variety']"],"execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.10, random_state=42)"],"execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["extras = ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}', 'cab',\"%\"]\n","from nltk.corpus import stopwords\n","stop = set(stopwords.words('english'))\n","stop.update(extras)"],"execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def pre_process(text):\n","    text = text.apply(lambda x: x.replace('/',''))                        #Replacing the / with none\n","    text = text.apply(lambda x: re.sub(\"  \",\" \", x))          #Replacing double space with single space\n","    text = text.apply(lambda x: re.sub(r'''[-()\\\"#/@;:{}`+=~|.!?,']''', \"\", x))     #Replacing special character with none\n","    text = text.apply(lambda x: re.sub(r'[0-9]+', '', x))                        #Replacing numbers with none\n","    text = text.apply(lambda x: \" \".join(x.translate(str.maketrans('', '', string.punctuation)) for x in x.split() if x.isalpha()))\n","    text = text.apply(lambda x: \" \".join(PorterStemmer().stem(word) for word in x.split())) #Stemming using porter stemmer\n","    text = text.apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))   #lemmatization\n","    return(text)"],"execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["X_train.review_description = pre_process(X_train.review_description)\n","X_val.review_description = pre_process(X_val.review_description)"],"execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["from scipy.sparse import hstack\n","\n","# vect = CountVectorizer(analyzer='word', \n","#                         token_pattern=r'\\w{1,}', \n","#                         ngram_range=(1, 2 ), \n","#                         min_df=5, \n","#                         encoding='latin-1' ,\n","#                         max_features=800,\n","#                         stop_words = stop)\n","\n","tfidf_vect = TfidfVectorizer()\n","X_train_dtm = tfidf_vect.fit_transform(X_train.review_description)\n","X_val_dtm = tfidf_vect.transform(X_val.review_description)\n","\n","price_train = X_train['price'].values[:,None]\n","price_val = X_val['price'].values[:,None]\n","points_train = X_train['points'].values[:,None]\n","points_val = X_val['points'].values[:,None]\n","country_train = X_train[encode_cols].values\n","country_val = X_val[encode_cols].values\n","X_train_dtm = hstack((X_train_dtm,country_train, price_train, points_train))\n","X_val_dtm = hstack((X_val_dtm, country_val, price_val, points_val))"],"execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def pre_process(X, y, tfidf_vect):\n","    dtm = tfidf_vect."],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["X_train_dtm"],"execution_count":34,"outputs":[{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"<65094x23234 sparse matrix of type '<class 'numpy.float64'>'\n\twith 2371326 stored elements in COOrdinate format>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":["le = LabelEncoder()\n","y_train = le.fit_transform(y_train)\n","y_val = le.transform(y_val)"],"execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["import xgboost as xgb"],"execution_count":51,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["estimator_function = xgb.XGBClassifier(max_depth=16,\n","                                    min_child_weight=10,\n","                                           learning_rate= 0.2,\n","                                           n_estimators= 1000,\n","                                           objective='multi:softmax',\n","                                           num_class= 28,\n","                                           nthread=4,\n","                                           n_jobs=4,\n","                                           seed=42)"],"execution_count":54,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["estimator_function.fit(X_train_dtm, y_train)"],"execution_count":55,"outputs":[{"output_type":"execute_result","execution_count":55,"data":{"text/plain":"XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n              importance_type='gain', interaction_constraints=None,\n              learning_rate=0.2, max_delta_step=0, max_depth=16,\n              min_child_weight=10, missing=nan, monotone_constraints=None,\n              n_estimators=1000, n_jobs=4, nthread=4, num_class=28,\n              num_parallel_tree=1, objective='multi:softprob', random_state=42,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=None, seed=42,\n              subsample=1, tree_method=None, validate_parameters=False,\n              verbosity=None)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":["pred = estimator_function.predict(X_val_dtm)"],"execution_count":58,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["import pickle\n","with open('xgb.pkl', 'wb') as file:\n","    pickle.dump(estimator_function, file)"],"execution_count":61,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["print(classification_report(y_val, pred, target_names=le.classes_))"],"execution_count":59,"outputs":[{"output_type":"stream","text":"                            precision    recall  f1-score   support\n\n  Bordeaux-style Red Blend       0.76      0.75      0.75       444\nBordeaux-style White Blend       0.51      0.42      0.46        50\n            Cabernet Franc       0.71      0.39      0.50        96\n        Cabernet Sauvignon       0.67      0.72      0.69       719\n           Champagne Blend       0.63      0.68      0.65        72\n                Chardonnay       0.79      0.86      0.82       844\n                     Gamay       0.77      0.61      0.68        67\n            Gewürztraminer       0.70      0.55      0.62        80\n          Grüner Veltliner       0.80      0.92      0.86        76\n                    Malbec       0.70      0.62      0.65       190\n                    Merlot       0.63      0.46      0.53       246\n                  Nebbiolo       0.81      0.86      0.83       178\n              Pinot Grigio       0.68      0.61      0.65        67\n                Pinot Gris       0.66      0.49      0.56       104\n                Pinot Noir       0.77      0.85      0.81       961\n            Portuguese Red       0.93      0.98      0.96       174\n          Portuguese White       0.86      0.88      0.87        69\n                 Red Blend       0.70      0.67      0.69       662\n     Rhône-style Red Blend       0.75      0.77      0.76       109\n                  Riesling       0.84      0.82      0.83       365\n                      Rosé       0.73      0.74      0.73       250\n                Sangiovese       0.57      0.49      0.53       166\n           Sauvignon Blanc       0.69      0.72      0.71       350\n           Sparkling Blend       0.66      0.65      0.65       126\n                     Syrah       0.64      0.62      0.63       295\n               Tempranillo       0.70      0.76      0.73       136\n               White Blend       0.75      0.64      0.69       143\n                 Zinfandel       0.67      0.70      0.69       194\n\n                  accuracy                           0.73      7233\n                 macro avg       0.72      0.69      0.70      7233\n              weighted avg       0.73      0.73      0.73      7233\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":["accuracy_score(y_val, pred)*100"],"execution_count":60,"outputs":[{"output_type":"execute_result","execution_count":60,"data":{"text/plain":"73.30291718512373"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import GridSearchCV"],"execution_count":62,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["#Choose best parameters for randomforest\n","\n","def best_params(train_x, train_y):\n","    rfc = RandomForestClassifier()\n","    param_grid = { \n","        'n_estimators': [50, 200],\n","        'max_features': ['auto', 'sqrt', 'log2']\n","    }\n","    \n","    CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n","    CV_rfc.fit(X_train_dtm, y_train)\n","    return CV_rfc.best_params_\n","\n","print(best_params(X_train_dtm, y_train))"],"execution_count":66,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["rf = RandomForestClassifier(n_estimators=200, max_features='auto', n_jobs=-1).fit(X_train_dtm, y_train)\n","print('Cross Validation for RandomForestClassifier')\n","print(rf.score(X_val_dtm, y_val))\n"],"execution_count":64,"outputs":[{"output_type":"stream","text":"Cross Validation for RandomForestClassifier\n0.6817364855523296\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":["import pickle\n","with open('rf.pkl', 'wb') as file:\n","    pickle.dump(rf, file)"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## LGBMC CLASSIFIER"]},{"metadata":{"trusted":true},"cell_type":"code","source":["import lightgbm as lgb"],"execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["d_train = lgb.Dataset(X_train_dtm.toarray(), label=y_train)"],"execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["params = {}\n","params['learning_rate'] = 0.2\n","params['boosting_type'] = 'gbdt'\n","params['n_estimators'] = 500\n","params['objective'] = 'multiclass'\n","params['metric'] = 'multi_logloss'\n","params['sub_feature'] = 0.5\n","params['num_leaves'] = 150\n","params['min_data'] = 50\n","params['max_depth'] = 10\n","params['num_class'] = 28\n","params['n_jobs'] = -1"],"execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["clf = lgb.train(params, d_train, 200)"],"execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["pred = clf.predict(X_val_dtm.toarray())"],"execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["ans = []\n","for arr in pred:\n","    ans.append(arr.argmax())\n","ans = np.array(ans)"],"execution_count":46,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["ans"],"execution_count":47,"outputs":[{"output_type":"execute_result","execution_count":47,"data":{"text/plain":"array([10, 14,  5, ..., 14, 20, 21])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":["accuracy_score(y_val, ans)*100"],"execution_count":48,"outputs":[{"output_type":"execute_result","execution_count":48,"data":{"text/plain":"74.18775058758467"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":["import pickle\n","with open('LGBMC.pkl', 'wb') as file:\n","    pickle.dump(clf, file)"],"execution_count":50,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## Working on test set"]},{"metadata":{"trusted":false},"cell_type":"code","source":["test.review_description = pre_process(test.review_description)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":["X_test_dtm = tfidf_vect.transform(test.review_description)\n","\n","price_test = test['price'].values[:,None]\n","X_test_dtm = hstack((X_test_dtm, price_test))"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":["test_pred = estimator_function.predict(X_test_dtm)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":["test_var_pred = le.inverse_transform(test_pred)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":["test_var_pred.size"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":["test.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":["## BAYESIAN OPTIMIZATION WAS TAKING TOO LONG WASN'T PERFORMED"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def xgbc_cv(max_depth,learning_rate,n_estimators,reg_alpha):\n","    from sklearn.metrics import roc_auc_score\n","    import numpy as np\n","    \n","    estimator_function = xgb.XGBClassifier(max_depth=int(max_depth),\n","                                           learning_rate= learning_rate,\n","                                           #min_child_weight = 10,\n","                                           n_estimators= int(n_estimators),\n","                                           reg_alpha = reg_alpha,\n","                                           nthread = -1,\n","                                           objective='multi:softmax',\n","                                           num_class= 28,\n","                                           n_jobs=-1,\n","                                           seed=42)\n","    # Fit the estimator\n","\n","    estimator_function.fit(X_train_dtm, y_train)\n","    \n","    # calculate out-of-the-box roc_score using validation set 1\n","    pred = estimator_function.predict(X_val_dtm)\n","    accuracy_score(y_val ,pred)\n","    \n","    # return the mean validation score to be maximized \n","    return accuracy_score(y_val ,pred)"]},{"metadata":{"trusted":true},"cell_type":"code","source":["from bayes_opt import BayesianOptimization\n","gp_params = {\"alpha\": 1e-10}\n","\n","\n","\n","hyperparameter_space = {\n","    'max_depth': (5, 20),\n","    'learning_rate': (0, 0.3),\n","    'n_estimators' : (500,1000),\n","    'reg_alpha': (0,1)\n","}\n","\n","xgbcBO = BayesianOptimization(f = xgbc_cv, \n","                             pbounds =  hyperparameter_space,\n","                             random_state = 42,\n","                             verbose = 10)\n","\n","# Finally we call .maximize method of the optimizer with the appropriate arguments\n","# kappa is a measure of 'aggressiveness' of the bayesian optimization process\n","# The algorithm will randomly choose 3 points to establish a 'prior', then will perform \n","# 10 interations to maximize the value of estimator function\n","xgbcBO.maximize(init_points=3,n_iter=10,acq='ucb', kappa= 3, **gp_params)"],"execution_count":null,"outputs":[]}],"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3-final"},"kernelspec":{"name":"python37364bitbasecondafec4e7b4c0214589abe96eee1d534749","display_name":"Python 3.7.3 64-bit ('base': conda)"}},"nbformat":4,"nbformat_minor":4}